# ğŸ“¦ Pipeline ETL Dá»¯ Liá»‡u BÃ¡n HÃ ng HÃ ng NgÃ y trÃªn Google Cloud Platform (GCP)

## Tá»•ng Quan Dá»± Ãn

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t **pipeline ETL serverless end-to-end** Ä‘á»ƒ xá»­ lÃ½ file CSV bÃ¡n hÃ ng hÃ ng ngÃ y, chuyá»ƒn Ä‘á»•i vÃ  lÃ m sáº¡ch dá»¯ liá»‡u, sau Ä‘Ã³ náº¡p vÃ o BigQuery Ä‘á»ƒ bÃ¡o cÃ¡o vÃ  phÃ¢n tÃ­ch. ToÃ n bá»™ quy trÃ¬nh Ä‘Æ°á»£c tá»± Ä‘á»™ng hÃ³a báº±ng cÃ¡c dá»‹ch vá»¥ quáº£n lÃ½ cá»§a GCP, Ä‘áº£m báº£o kháº£ nÄƒng má»Ÿ rá»™ng, váº­n hÃ nh á»•n Ä‘á»‹nh vÃ  tiáº¿t kiá»‡m chi phÃ­.

---

## SÆ¡ Äá»“ Kiáº¿n TrÃºc

```
[CSV Data] â†’ [Cloud Storage] â†’ [Cloud Scheduler] â†’ [Cloud Workflows] â†’ [Cloud Run] â†’ [BigQuery] â†’ [Looker Studio]
```

- **Cloud Storage (GCS):** LÆ°u trá»¯ cÃ¡c file CSV bÃ¡n hÃ ng hÃ ng ngÃ y.
- **Cloud Run:** Cháº¡y script ETL Python trong container (Flask web server).
- **Cloud Workflows:** Äiá»u phá»‘i cÃ¡c bÆ°á»›c pipeline qua HTTP call.
- **Cloud Scheduler:** LÃªn lá»‹ch cháº¡y hÃ ng ngÃ y.
- **BigQuery:** LÆ°u trá»¯ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»ƒ truy váº¥n vÃ  táº¡o dashboard.
- **Looker Studio:** Táº¡o dashboard tá»« dá»¯ liá»‡u BigQuery.

---

## ThÃ nh Pháº§n & CÃ¡c BÆ°á»›c Triá»ƒn Khai

### 1. Thiáº¿t Láº­p Google Cloud

XÃ¡c thá»±c vÃ  chá»n project:
```bash
gcloud auth login
gcloud config set project interns-2025-467409
```

### 2. Báº­t CÃ¡c API Cáº§n Thiáº¿t

```bash
gcloud services enable \
  cloudbuild.googleapis.com \
  run.googleapis.com \
  workflows.googleapis.com \
  cloudscheduler.googleapis.com \
  bigquery.googleapis.com \
  storage.googleapis.com
```

### 3. Cloud Storage (GCS)

- **Bucket:** `sales-bucket-ta-1-group3` (`asia-southeast1`)
- **Upload Dá»¯ Liá»‡u:**
  ```bash
  gcloud storage buckets create gs://sales-bucket-ta-1-group3 --location=asia-southeast1
  gcloud storage cp data/retail_sales.csv gs://sales-bucket-ta-1-group3/sales.csv
  gcloud storage ls gs://sales-bucket-ta-1-group3/
  ```
- **Má»¥c Ä‘Ã­ch:** LÆ°u trá»¯ táº­p trung cÃ¡c file CSV Ä‘áº§u vÃ o.

### 4. Service Account

- **Email:** `service-account-group-3-nexlab@interns-2025-467409.iam.gserviceaccount.com`
- **Quyá»n:** Storage, BigQuery, Cloud Run, Workflows (OIDC Ä‘á»ƒ gá»i dá»‹ch vá»¥ an toÃ n).

### 5. Build Docker & Deploy Cloud Run

- **Build Container:**
  ```bash
  gcloud builds submit --config=infra/cloudbuild.yaml .
  ```
  - Copy mÃ£ nguá»“n, cÃ i dependencies (`pandas`, `google-cloud-*`, `flask`, `pyarrow`), build Docker image.
  - **Image:** `gcr.io/interns-2025-467409/etl-sales-pipeline:latest`
- **Deploy Cloud Run Service:**
  ```bash
  gcloud run deploy etl-sales-pipeline \
    --image gcr.io/interns-2025-467409/etl-sales-pipeline:latest \
    --region asia-southeast1 \
    --platform managed \
    --allow-unauthenticated \
    --memory 2Gi \
    --cpu 1 \
    --timeout 3600 \
    --max-instances 1 \
    --set-env-vars PROJECT_ID=interns-2025-467409
  ```
  - **Endpoints:**
    - `/` - Trigger ETL pipeline
    - `/health` - Kiá»ƒm tra sá»©c khá»e
    - `/test` - Kiá»ƒm tra tráº¡ng thÃ¡i

### 6. Cloud Workflows

- **Triá»ƒn khai Workflow:**
  ```bash
  gcloud workflows deploy etl-sales-workflow \
    --source=infra/workflow.yaml \
    --location=asia-southeast1
  ```
  - Äiá»u phá»‘i gá»i Cloud Run.
  - Xá»­ lÃ½ lá»—i vÃ  ghi log lÃªn Cloud Logging.

### 7. Cloud Scheduler

- **Táº¡o Scheduler Job:**
  ```bash
  gcloud scheduler jobs create http daily-etl-sales-pipeline \
    --location=asia-southeast1 \
    --schedule="0 9 * * *" \
    --time-zone="Asia/Saigon" \
    --uri="https://workflows.googleapis.com/v1/projects/interns-2025-467409/locations/asia-southeast1/workflows/etl-sales-workflow/executions" \
    --http-method=POST \
    --oidc-service-account-email=service-account-group-3-nexlab@interns-2025-467409.iam.gserviceaccount.com \
    --headers="Content-Type=application/json" \
    --message-body="{}" \
    --description="Daily ETL pipeline for sales data processing"
  ```
  - **Lá»‹ch:** HÃ ng ngÃ y lÃºc 9:00 AM ICT (GMT+7).

---

## Quy TrÃ¬nh ETL

### CÃ¡c BÆ°á»›c ChÃ­nh

1. **Extract:** Táº£i file CSV tá»« Cloud Storage.
2. **Transform:**
   - Loáº¡i bá» giÃ¡ trá»‹ null
   - Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u (ngÃ y, sá»‘)
   - Chuáº©n hÃ³a format (thÃªm cá»™t ThÃ¡ng/NÄƒm)
   - Loáº¡i bá» trÃ¹ng láº·p (theo Transaction ID)
3. **Load:** Náº¡p dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ vÃ o BigQuery (`WRITE_APPEND`).

### CÃ¡c HÃ m ChÃ­nh (`etl/main.py`)

- `clean_data()` - LÃ m sáº¡ch vÃ  chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u
- `create_bucket()` - Quáº£n lÃ½ GCS bucket
- `create_dataset_bq()` - Táº¡o dataset/table BigQuery
- `download_blob()` - Táº£i dá»¯ liá»‡u tá»« GCS
- `load_to_bq()` - Náº¡p dá»¯ liá»‡u vÃ o BigQuery
- `run_etl()` - Äiá»u phá»‘i toÃ n bá»™ ETL
- CÃ¡c route Flask: `/`, `/health`, `/test`

---

## Báº£o Máº­t & XÃ¡c Thá»±c

- **Service Account:** DÃ¹ng cho má»i dá»‹ch vá»¥, gá»i nhau qua OIDC token.
- **PhÃ¢n quyá»n:** Tá»‘i thiá»ƒu cáº§n thiáº¿t (Storage, BigQuery, Workflows, Cloud Run).
- **Workflow call:** XÃ¡c thá»±c audience/endpoint Ä‘Ãºng.

---

## Káº¿t Quáº£ & Kiá»ƒm Tra

- **ETL Pipeline:** 1000+ dÃ²ng xá»­ lÃ½ vÃ  náº¡p vÃ o BigQuery.
- **Cloud Run:** Endpoint hoáº¡t Ä‘á»™ng vÃ  kiá»ƒm tra tá»‘t.
- **Workflows:** Thá»i gian cháº¡y trung bÃ¬nh 8.493s, cÃ³ xá»­ lÃ½ lá»—i.
- **BigQuery:** Dá»¯ liá»‡u truy váº¥n Ä‘Æ°á»£c, sáºµn sÃ ng cho dashboard.
- **Scheduler:** Cron hoáº¡t Ä‘á»™ng, kiá»ƒm tra trigger thÃ nh cÃ´ng.
- **Container:** Build á»•n Ä‘á»‹nh, Ä‘Ã£ fix dependency.

**VÃ­ dá»¥ pháº£n há»“i thÃ nh cÃ´ng:**
```json
{
  "message": "ETL process completed successfully.",
  "rows_processed": 1000,
  "status": "success", 
  "table": "interns-2025-467409.sales_dataset.sales_table"
}
```

---

## Theo DÃµi & Logging

- **Cloud Logging:** Ghi láº¡i cÃ¡c bÆ°á»›c pipeline, lá»—i Ä‘á»ƒ dá»… kiá»ƒm tra.
- **Health Check:** DÃ¹ng endpoint `/health` Ä‘á»ƒ monitor runtime.

---

## Cáº¥u TrÃºc MÃ£ Nguá»“n

```
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ main.py          # Logic ETL & Flask app
â”‚   â”œâ”€â”€ requirements.txt # ThÆ° viá»‡n Python
â”‚   â”œâ”€â”€ utils.py         # HÃ m phá»¥ trá»£
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile       # Cáº¥u hÃ¬nh container
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ workflow.yaml    # Äiá»u phá»‘i Cloud Workflow
â”‚   â””â”€â”€ cloudbuild.yaml  # Tá»± Ä‘á»™ng hÃ³a Cloud Build
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sales_data.csv   # Dá»¯ liá»‡u máº«u
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py        # Biáº¿n cáº¥u hÃ¬nh & env
```

---

## Ghi chÃº & LÆ°u Ã½

- **Port 8080:** Cloud Run yÃªu cáº§u container pháº£i listen cá»•ng nÃ y.
- **Lá»—i xÃ¡c thá»±c:** Äáº£m báº£o service account Ä‘á»§ quyá»n, OIDC setup Ä‘Ãºng.
- **Format YAML:** Ráº¥t nháº¡y vá» thá»¥t Ä‘áº§u dÃ²ng.
- **Tá»‘i Æ°u BigQuery:** DÃ¹ng partition & clustering (bonus).
- **LÆ°u trá»¯/ThÃ´ng bÃ¡o:** Di chuyá»ƒn file Ä‘Ã£ xá»­ lÃ½ sang folder archive hoáº·c gá»­i email qua Pub/Sub + Cloud Functions (bonus, nÃªn Ã¡p dá»¥ng sáº£n xuáº¥t).

---

## Tráº¡ng ThÃ¡i Há»‡ Thá»‘ng

**PRODUCTION READY - TOÃ€N Bá»˜ Há»† THá»NG ÄÃƒ HOáº T Äá»˜NG**

- Code, háº¡ táº§ng & tá»± Ä‘á»™ng hÃ³a Ä‘Ã£ triá»ƒn khai vÃ  test
- CÃ³ monitoring & xá»­ lÃ½ lá»—i
- ETL tá»± Ä‘á»™ng cháº¡y má»—i ngÃ y lÃºc **9:00 AM ICT**

---