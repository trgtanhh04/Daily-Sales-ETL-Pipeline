# SALES DATA ETL PIPELINE - COMPLETE DOCUMENTATION
# NgÃ y hoÃ n thÃ nh: 8/8/2025
# Status: âœ… PRODUCTION READY

## ğŸ“‹ Tá»”NG QUAN KIáº¾N TRÃšC
```
[CSV Data] â†’ [Cloud Storage] â†’ [Cloud Scheduler] â†’ [Cloud Workflows] â†’ [Cloud Run] â†’ [BigQuery] â†’ [Looker Studio]
```

## ğŸ”§ CÃC THÃ€NH PHáº¦N ÄÃƒ TRIá»‚N KHAI THÃ€NH CÃ”NG

### 1. CLOUD STORAGE âœ…
- Bucket: `sales-bucket-ta-1-group3`
- Region: `asia-southeast1`
- File: `sales.csv` (1000 rows data)
- Status: Active & Tested

### 2. CLOUD RUN SERVICE âœ…
- Service Name: `etl-sales-pipeline`
- URL: `https://etl-sales-pipeline-304934213711.asia-southeast1.run.app/`
- Region: `asia-southeast1`
- Image: `gcr.io/interns-2025-467409/etl-sales-pipeline:latest`
- Port: 8080 (Flask web server)
- Revision: `00011-qjc`
- Status: Active & Processing Data

#### Endpoints:
- `/` - Trigger ETL pipeline (GET/POST) âœ… Tested
- `/health` - Health check âœ… Working
- `/test` - Service status âœ… Working

### 3. BIGQUERY âœ…
- Project: `interns-2025-467409`
- Dataset: `sales_dataset`
- Table: `sales_table`
- Location: `asia-southeast1`
- Rows Processed: 1000+ (successfully loaded)
- Status: Active & Receiving Data

### 4. CLOUD WORKFLOWS âœ…
- Workflow Name: `etl-sales-workflow`
- Location: `asia-southeast1`
- File: `infra/workflow.yaml` (52 lines)
- Execution Time: 8.493 seconds âœ… Tested
- Status: Deployed & Operational

### 5. CLOUD SCHEDULER âœ…
- Job Name: `daily-etl-sales-pipeline`
- Schedule: `0 9 * * *` (Daily at 9:00 AM)
- Timezone: Indochina Time (ICT) / GMT+7
- Target: Cloud Workflows API
- Auth: OIDC Token (service-account-group-3-nexlab)
- Status: Configured & Ready

## ğŸ“Š ETL PIPELINE THá»°C Táº¾

### Data Flow Ä‘Ã£ test thÃ nh cÃ´ng:
1. **Extract**: Download CSV tá»« Cloud Storage âœ…
2. **Transform**: 
   - Remove null values âœ…
   - Convert data types (Date, Age, Quantity, Price, Amount) âœ…
   - Add Month/Year columns âœ…
   - Remove duplicates by Transaction ID âœ…
3. **Load**: Insert vÃ o BigQuery vá»›i WRITE_APPEND âœ…

### Automation Flow hoáº¡t Ä‘á»™ng:
```
Daily 9:00 AM ICT â† Cloud Scheduler
    â†“
Cloud Workflows executes â† 8.5s execution time
    â†“
HTTP POST to Cloud Run â† Flask endpoints
    â†“
ETL processes 1000 rows â† Data transformation
    â†“
BigQuery receives data â† Successfully loaded
    â†“
Success response returned â† JSON status
```

## ğŸš€ DEPLOYMENT HISTORY

### Build Process:
- Docker builds: 7 iterations (Build #1-7)
- Final deployment: Successful
- Container Registry: Image active
- Cloud Run: Service running on revision 00011-qjc

### Issues Resolved:
1. **PORT 8080**: Added Flask web server âœ…
2. **Import conflicts**: Consolidated files âœ…
3. **BigQuery naming**: Fixed dataset names âœ…
4. **Workflow timeout**: Optimized to 1800s âœ…
5. **YAML syntax**: Fixed formatting âœ…

## ğŸ“ FINAL CODE STRUCTURE

### Core Files:
- `etl/main.py` â† Main ETL (120 lines, all functions consolidated)
- `docker/Dockerfile` â† Container config
- `infra/workflow.yaml` â† Orchestration (52 lines)
- `infra/cloudbuild.yaml` â† Build automation
- `data/sales_data.csv` â† Source data

### Key Functions (main.py):
- `clean_data()` â† Data cleaning & transformation
- `create_bucket()` â† GCS management
- `create_dataset_bq()` â† BigQuery setup
- `download_blob()` â† Data extraction
- `load_to_bq()` â† Data loading
- `run_etl()` â† Main orchestration
- Flask routes: `/`, `/health`, `/test`

## ğŸ” SECURITY & AUTHENTICATION

### Service Account Used:
- `service-account-group-3-nexlab@interns-2025-467409.iam.gserviceaccount.com`
- Permissions: Workflows, Cloud Run, Storage, BigQuery
- OIDC Token Authentication
- Audience: Workflows API endpoint

## ğŸ“ˆ TESTING RESULTS

### Successfully Validated:
âœ… ETL Pipeline: 1000 rows processed correctly
âœ… Cloud Run: All endpoints responding
âœ… Workflows: 8.493s execution (SUCCESS state)
âœ… BigQuery: Data loaded and queryable  
âœ… Docker: Container running stable
âœ… Authentication: OIDC tokens working
âœ… Scheduling: Cron expression validated

## ğŸ¯ FINAL STATUS

**ğŸŸ¢ PRODUCTION READY - ALL SYSTEMS OPERATIONAL**

- âœ… Code deployed and tested
- âœ… Infrastructure provisioned  
- âœ… Automation configured
- âœ… Data pipeline flowing
- âœ… Monitoring available
- âœ… Error handling implemented

**Daily ETL sáº½ cháº¡y tá»± Ä‘á»™ng lÃºc 9:00 AM ICT**

---

## SETUP & DEPLOYMENT LOG (Chi tiáº¿t ká»¹ thuáº­t)

### 1. Google Cloud Setup
```bash
$ gcloud auth login
$ gcloud config set project interns-2025-467409
$ gcloud config list
```

### 2. APIs & Services Enabled
```bash
$ gcloud services enable cloudbuild.googleapis.com run.googleapis.com workflows.googleapis.com cloudscheduler.googleapis.com bigquery.googleapis.com storage.googleapis.com
```

### 3. GCS Bucket Setup
```bash
# Táº¡o bucket
$ gcloud storage buckets create gs://sales-bucket-ta-1-group3 --location=asia-southeast1

# Upload data
$ gcloud storage cp data\retail_sales.csv gs://sales-bucket-ta-1-group3/sales.csv

# Verify
$ gcloud storage ls gs://sales-bucket-ta-1-group3/
```

### 4. Service Account 
- ÄÃ£ cÃ³ sáºµn: `service-account-group-3-nexlab@interns-2025-467409.iam.gserviceaccount.com`
- Roles: CustomRole675, aiplatform.user

### 5. Docker Build & Deploy
```bash
# Build Docker image thÃ nh cÃ´ng
$ gcloud builds submit --config=infra/cloudbuild-simple.yaml .
# Result: SUCCESS - Image: gcr.io/interns-2025-467409/etl-sales-pipeline:latest
```

### 6. Cloud Run Deploy Issues
- Láº§n 1: Deploy service failed - Container khÃ´ng listen PORT 8080
- Giáº£i phÃ¡p: ThÃªm Flask web server HOáº¶C sá»­ dá»¥ng Cloud Run Jobs

### 7. Config Updates
- Updated BUCKET_NAME: "sales-bucket-ta-1-group3"  
- Updated PATH_TO_FILE: '../data/retail_sales.csv'
- Added SERVICE_ACCOUNT_EMAIL trong config

```
## QUY TRÃŒNH DEPLOY CHI TIáº¾T

### BÆ¯á»šC 1: BUILD DOCKER IMAGE
```bash
gcloud builds submit --config=infra/cloudbuild.yaml .
```
**Giáº£i thÃ­ch:**
- Äá»c file `cloudbuild.yaml` Ä‘á»ƒ biáº¿t cÃ¡ch build
- Copy toÃ n bá»™ code tá»« local vÃ o Docker container  
- Cháº¡y `docker build -f docker/Dockerfile` 
- Install dependencies tá»« `etl/requirements.txt`
- Táº¡o image: `gcr.io/interns-2025-467409/etl-sales-pipeline:latest`
- Push image lÃªn Google Container Registry
- **Káº¿t quáº£:** Docker image sáºµn sÃ ng Ä‘á»ƒ deploy

### BÆ¯á»šC 2: DEPLOY CLOUD RUN SERVICE  
```bash
gcloud run deploy etl-sales-pipeline \
  --image gcr.io/interns-2025-467409/etl-sales-pipeline:latest \
  --region asia-southeast1 \
  --platform managed \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 1 \
  --timeout 3600 \
  --max-instances 1 \
  --set-env-vars PROJECT_ID=interns-2025-467409


gcloud run deploy etl-sales-pipeline --image gcr.io/interns-2025-467409/etl-sales-pipeline:latest --region asia-southeast1 --platform managed --allow-unauthenticated --memory 2Gi --cpu 1 --timeout 3600 --max-instances 1 --set-env-vars PROJECT_ID=interns-2025-467409
```
**Giáº£i thÃ­ch:**
- Láº¥y Docker image tá»« Container Registry
- Táº¡o Cloud Run service (serverless container)
- Service cháº¡y `main.py` â†’ Flask web server trÃªn PORT 8080
- Expose HTTP endpoints: `/`, `/health`, `/test`
- **Káº¿t quáº£:** Web service sáºµn sÃ ng nháº­n HTTP requests

### BÆ¯á»šC 3: TRIGGER ETL PIPELINE
```bash
curl -X GET "https://etl-sales-pipeline-304934213711.asia-southeast1.run.app/"
```
**Giáº£i thÃ­ch:**
- Gá»i endpoint `/` cá»§a Cloud Run service
- Flask app nháº­n request â†’ gá»i function `run_etl()`
- ETL process: Download CSV tá»« GCS â†’ Clean data â†’ Load vÃ o BigQuery
- **Káº¿t quáº£:** Data Ä‘Æ°á»£c xá»­ lÃ½ vÃ  lÆ°u vÃ o BigQuery

### BÆ¯á»šC 4: SETUP CLOUD WORKFLOWS (TÃ¹y chá»n)
```bash
gcloud workflows deploy etl-sales-workflow \
  --source=infra/workflow.yaml \
  --location=asia-southeast1
```
**Giáº£i thÃ­ch:**
- Deploy workflow Ä‘á»ƒ orchestrate ETL pipeline
- Workflow gá»i Cloud Run service theo schedule
- CÃ³ error handling vÃ  logging
- **Káº¿t quáº£:** Automation cho ETL pipeline

### BÆ¯á»šC 5: SETUP CLOUD SCHEDULER (TÃ¹y chá»n)
```bash
gcloud scheduler jobs create http etl-daily-job \
  --schedule="0 9 * * *" \
  --uri="https://etl-sales-pipeline-304934213711.asia-southeast1.run.app/" \
  --location=asia-southeast1
```
**Giáº£i thÃ­ch:**
- Táº¡o cron job cháº¡y hÃ ng ngÃ y lÃºc 9AM
- Tá»± Ä‘á»™ng trigger ETL pipeline
- ### ğŸ‰ ETL PIPELINE SUCCESS! (Build #7)
âœ… **Build Docker:** SUCCESS vá»›i pyarrow dependency  
âœ… **Deploy Cloud Run:** SUCCESS (revision 00011-qjc)  
âœ… **Flask Web Server:** Running on PORT 8080  
âœ… **Health Check:** Working `/health`  
âœ… **Debug Endpoint:** Working `/test`  
âœ… **ETL Pipeline:** SUCCESS! 1,000 rows processed

**ETL Test Result:**
```json
{
  "message": "ETL process completed successfully.",
  "rows_processed": 1000,
  "status": "success", 
  "table": "interns-2025-467409.sales_dataset.sales_table"
}
```

**Complete Workflow:**
1. âœ… Download CSV from GCS bucket `sales-bucket-ta-1-group3/sales.csv`
2. âœ… Clean and transform data (1,000 records) 
3. âœ… Load to BigQuery table `interns-2025-467409.sales_dataset.sales_table`

**Architecture:**
- **Main File:** `etl/main.py` (Flask app vá»›i full ETL logic)
- **Dockerfile:** `docker/Dockerfile` â†’ runs `etl/main.py`
- **Dependencies:** `etl/requirements.txt` (pandas, google-cloud-*, flask, pyarrow)
- **Config:** `config/config.py` vá»›i fallback environment variables
- **Utils:** `etl/utils.py` vá»›i fallback implementations

**Cleaned up:** Removed simple_main.py vÃ  Dockerfile.simple (khÃ´ng cáº§n thiáº¿t)

**Service URL:** https://etl-sales-pipeline-304934213711.asia-southeast1.run.app

**Next Steps:**
1. âœ… ETL Pipeline fully functional
2. âœ… Setup Cloud Workflows automation (SUCCESS - 8.493s execution)
3. [ ] Configure Cloud Scheduler for daily runs
4. [ ] Build Looker Studio dashboard
5. [ ] Verify BigQuery data

### ğŸ‰ CLOUD WORKFLOWS SUCCESS!
```bash
# Deployed successfully:
$ gcloud workflows deploy etl-sales-workflow --source=infra/workflow.yaml --location=asia-southeast1

# Workflow Details:
- Name: etl-sales-workflow
- Location: asia-southeast1  
- State: ACTIVE
- Latest Execution: SUCCEEDED (8.493 seconds)
- Timeout: Fixed to 1800s (30 minutes)

# Execution Results:
âœ… workflow_end: Succeeded 
âœ… Full orchestration working
âœ… ETL pipeline triggered via workflow
```

**Workflow Features:**
âœ… Orchestrates ETL pipeline via HTTP calls  
âœ… Error handling with success/failure logging
âœ… Cloud Logging integration
âœ… Ready for Cloud Scheduler integration
âœ… **TESTED & WORKING** ğŸš€


# Táº¡o schedule:
gcloud scheduler jobs create http daily-etl-sales-pipeline --location=asia-southeast1 --schedule="0 9 * * *" --time-zone="Asia/Saigon" --uri="https://workflows.googleapis.com/v1/projects/interns-2025-467409/locations/asia-southeast1/workflows/etl-sales-workflow/executions" --http-method=POST --oidc-service-account-email=service-account-group-3-nexlab@interns-2025-467409.iam.gserviceaccount.com --headers="Content-Type=application/json" --message-body="{}" --description="Daily ETL pipeline for sales data processing"

# Force run scheduler job Ä‘á»ƒ test ngay
gcloud scheduler jobs run daily-etl-sales-pipeline --location=asia-southeast1

# XÃ³a schedule: 
gcloud scheduler jobs delete daily-etl-sales-pipeline --location=asia-southeast1 --quiet